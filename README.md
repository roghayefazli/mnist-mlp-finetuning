
# MNIST MLP Fine-Tuning

This repository contains a notebook for fine-tuning an **MLP (Multi-Layer Perceptron)** model on the **MNIST** handwritten digits dataset.  
The notebook includes:

- Loading and preprocessing MNIST data  
- Building and training an MLP model  
- Fine-tuning on custom data  
- Saving and loading trained models  
- Generating predictions  

---

## ğŸ“Œ Project Structure



---

## ğŸš€ Features

- Simple MLP architecture for digit classification  
- Fine-tuning workflow for improving performance  
- Clean and readable implementation  
- Easy to extend for experiments or custom datasets  

---

## ğŸ§  Model Overview

- Input: 28Ã—28 grayscale image  
- Layers: Dense hidden layers + ReLU  
- Optimizer: Adam  
- Loss: Sparse Categorical Crossentropy  
- Output: 10-class digit prediction (0â€“9)

---

## ğŸ“¦ Requirements

You can install required libraries with:

```bash
pip install tensorflow numpy matplotlib



â–¶ï¸ How to Run

Clone the repository:

git clone https://github.com/<your-username>/mnist-mlp-finetuning.git
cd mnist-mlp-finetuning


Open the notebook:

jupyter notebook MNIST_FineTuning.ipynb


Run all cells to train or fine-tune the model.

ğŸ“Š Results

The notebook prints accuracy and loss for training and test sets, and includes demonstration predictions.

âœ¨ Author

Roghayeh (Rogheye) Fazli
MSc Student in Artificial Intelligence



